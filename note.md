# Diffusion Forcing Language Model - é¡¹ç›®æ–‡æ¡£

## ğŸ“‹ é¡¹ç›®ç›®æ ‡

å°† **Diffusion Forcing** è®­ç»ƒæ–¹æ³•è¿ç§»åˆ° **nanoGPT** ä»£ç åº“ï¼Œå®ç°ä¸€ä¸ªå¯ä»¥ä¸æ ‡å‡† **Teacher Forcing** GPT è¿›è¡Œå…¬å¹³å¯¹æ¯”çš„è¯­è¨€æ¨¡å‹ã€‚

**æ ¸å¿ƒç›®æ ‡**ï¼š
- åœ¨ nanoGPT æ¡†æ¶å†…å®ç°å®Œæ•´çš„ Diffusion Forcing è®­ç»ƒå’Œé‡‡æ ·æµç¨‹
- ä¿æŒ nanoGPT çš„ä»£ç é£æ ¼å’Œç®€æ´æ€§
- å®ç°ä¸åŸç‰ˆ Diffusion Forcing ä»“åº“ï¼ˆ`diffusion-forcing/algorithms/diffusion_forcing`ï¼‰**å®Œå…¨ä¸€è‡´**çš„è®­ç»ƒ/é‡‡æ ·é€»è¾‘
- é€šè¿‡ä¸€ä¸ªç®€å•çš„å¼€å…³ï¼ˆ`use_diffusion_forcing`ï¼‰åˆ‡æ¢ Teacher Forcing å’Œ Diffusion Forcing

---

## ğŸ¯ æŠ€æœ¯èƒŒæ™¯ï¼šDiffusion Forcing æ ¸å¿ƒæ¦‚å¿µ

### ä»€ä¹ˆæ˜¯ Diffusion Forcingï¼Ÿ

Diffusion Forcing æ˜¯ä¸€ç§ç»“åˆäº† **next-token prediction** å’Œ **full-sequence diffusion** çš„è®­ç»ƒæ–¹æ³•ï¼š

1. **è®­ç»ƒé˜¶æ®µ**ï¼š
   - æ¯ä¸ª token ç‹¬ç«‹é‡‡æ ·ä¸€ä¸ª noise levelï¼ˆä¾‹å¦‚ `[5, 12, 3, 20, ...]`ï¼‰
   - åœ¨ **embedding ç©ºé—´**å¯¹æ¯ä¸ª token ç‹¬ç«‹åŠ å™ªï¼š`x_t[i] = q_sample(x0[i], noise_level[i], noise[i])`
   - æ¨¡å‹**ä¸€æ¬¡æ€§å¹¶è¡Œé¢„æµ‹æ•´ä¸ªåºåˆ—**ï¼Œä½†æ¯ä¸ªä½ç½®çœ‹åˆ°ä¸åŒçš„ noise levelï¼ˆé€šè¿‡æ—¶é—´åµŒå…¥ä¼ å…¥ï¼‰
   - ä½¿ç”¨ **Fused SNR Reweighting** è®¡ç®—æŸå¤±

2. **é‡‡æ ·é˜¶æ®µ**ï¼š
   - ä½¿ç”¨**è°ƒåº¦çŸ©é˜µ**ï¼ˆscheduling matrixï¼‰æ§åˆ¶ä¸åŒ token çš„å»å™ªè¿›åº¦ï¼ˆpyramid/trapezoid è°ƒåº¦ï¼‰
   - **æ»‘åŠ¨çª—å£**æœºåˆ¶ï¼šæ¯æ¬¡åªå¯¹æœ€å `n_tokens` ä¸ªä½ç½®è¿›è¡Œå»å™ª
   - å¤šæ­¥è¿­ä»£ï¼šé€šè¿‡è°ƒåº¦çŸ©é˜µçš„å¤šä¸ª rowï¼Œé€æ­¥å»å™ªæ•´ä¸ªåºåˆ—

### å…³é”®è®¾è®¡åŸåˆ™

âš ï¸ **é‡è¦ç†è§£**ï¼šDiffusion Forcing **ä¸æ˜¯**å•ä¸ª token é€ä¸ªå»å™ªï¼Œè€Œæ˜¯ï¼š
- **å¹¶è¡Œå¤š token å»å™ª**ï¼šæ¨¡å‹ä¸€æ¬¡æ€§å¤„ç†æ•´ä¸ªåºåˆ— `(B, T, C)`
- **ç‹¬ç«‹ noise level**ï¼šæ¯ä¸ª token æœ‰ç‹¬ç«‹çš„å™ªå£°çº§åˆ«ï¼Œä½“ç°"å› æœä¸ç¡®å®šæ€§"
- **è°ƒåº¦çŸ©é˜µ**ï¼šé‡‡æ ·æ—¶ï¼Œè¶Šè¿œçš„ token å™ªå£°è¶Šå¤§ï¼Œéœ€è¦æ›´å¤šå»å™ªæ­¥æ•°

---

## ğŸ”‘ å…³é”®å®ç°ç»†èŠ‚

### 1. æ¨¡å‹æ¶æ„ï¼š`DFGPT` ç±»

**ä½ç½®**ï¼š`nanoGPT/model.py`ï¼Œç»§æ‰¿è‡ª `GPT`

**æ ¸å¿ƒç»„ä»¶**ï¼š
- **å¤ç”¨ GPT ç»„ä»¶**ï¼š
  - `transformer.wte`ï¼štoken embedding
  - `transformer.wpe`ï¼šposition embedding
  - `transformer.h`ï¼šGPT transformer blocksï¼ˆä¿æŒå› æœæ³¨æ„åŠ›ï¼‰
  - `transformer.ln_f`ï¼šfinal layer norm

- **æ–°å¢ Diffusion ç»„ä»¶**ï¼š
  - `df_t_embed = SinusoidalPosEmb(n_embd)`ï¼šæ‰©æ•£æ—¶é—´æ­¥åµŒå…¥
  - `df_head = Linear(n_embd, n_embd)`ï¼šé¢„æµ‹å¤´ï¼Œå°† transformer è¾“å‡ºæ˜ å°„å› embedding ç©ºé—´
  - Diffusion ç¼“å†²åŒºï¼š`df_betas`, `df_alphas_cumprod`, `df_snr`, `df_clipped_snr` ç­‰

### 2. è®­ç»ƒæµç¨‹ï¼š`DFGPT.forward()`

**è¾“å…¥**ï¼š`idx: (B, T)` token indices

**æµç¨‹**ï¼š
```python
1. x0 = wte(idx) + wpe(pos)                    # å¹²å‡€ embedding (B, T, C)
2. noise_levels ~ Uniform[0, df_timesteps]    # æ¯ä¸ª token ç‹¬ç«‹é‡‡æ · (B, T)
3. noise ~ N(0, I)                             # é«˜æ–¯å™ªå£° (B, T, C)
4. x_t = q_sample(x0, noise_levels, noise)     # æ¯ä¸ªä½ç½®ç‹¬ç«‹åŠ å™ª
5. pred = _df_model_predictions(x_t, noise_levels)  # ä¸€æ¬¡æ€§é¢„æµ‹æ•´ä¸ªåºåˆ—
6. loss = MSE(pred, target) * FusedSNR(noise_levels)
```

**å…³é”®ç‚¹**ï¼š
- âœ… **ä¸€æ¬¡æ€§é¢„æµ‹æ•´ä¸ªåºåˆ—**ï¼š`_df_model_predictions` æ¥æ”¶ `(B, T, C)`ï¼Œè¾“å‡º `(B, T, C)`
- âœ… **æ¯ä¸ª token ç‹¬ç«‹çš„ noise level**ï¼šé€šè¿‡ `df_t_embed(noise_levels)` ä¼ å…¥
- âœ… **Fused SNR Reweighting**ï¼šæ²¿æ—¶é—´ç»´åº¦ï¼ˆtokenï¼‰åšæŒ‡æ•°æ»‘åŠ¨å¹³å‡

### 3. Fused SNR Reweighting

**ä½ç½®**ï¼š`DFGPT._compute_loss_weight()`

**æ•°å­¦å½¢å¼**ï¼ˆå®Œå…¨å¯¹åº”åŸç‰ˆ `Diffusion.compute_loss_weights`ï¼‰ï¼š
```python
# 1. å½’ä¸€åŒ– SNR
normalized_snr = snr / snr_clip
normalized_clipped_snr = clipped_snr / snr_clip

# 2. æ²¿æ—¶é—´ç»´åº¦ç´¯ç§¯ï¼ˆæŒ‡æ•°æ»‘åŠ¨å¹³å‡ï¼‰
cum_snr[t] = cum_snr_decay * cum_snr[t-1] + (1 - cum_snr_decay) * normalized_clipped_snr[t]

# 3. Fused SNR
fused_snr = 1 - (1 - cum_snr * cum_snr_decay) * (1 - normalized_snr)
clipped_fused_snr = 1 - (1 - cum_snr * cum_snr_decay) * (1 - normalized_clipped_snr)

# 4. æƒé‡ï¼ˆæ ¹æ® objectiveï¼‰
if pred_noise:
    weight = clipped_fused_snr / fused_snr
elif pred_x0:
    weight = clipped_fused_snr * snr_clip
elif pred_v:
    weight = clipped_fused_snr * snr_clip / (fused_snr * snr_clip + 1)
```

### 4. é‡‡æ ·æµç¨‹ï¼š`DFGPT.generate_df()`

**æ ¸å¿ƒé€»è¾‘**ï¼ˆå¯¹åº”åŸç‰ˆ `DiffusionForcingBase.validation_step`ï¼‰ï¼š

```python
1. åˆå§‹åŒ– context embeddings ä¸ºå¹²å‡€ x0
2. while curr_pos < total_len:
     a. ç”Ÿæˆè°ƒåº¦çŸ©é˜µï¼ˆpyramid/full_sequenceï¼‰
     b. åˆå§‹åŒ–æ–° chunk ä¸ºé«˜æ–¯å™ªå£°
     c. æ»‘åŠ¨çª—å£ï¼šstart_pos = max(0, curr_pos + horizon - block_size)
     d. å¯¹è°ƒåº¦çŸ©é˜µçš„æ¯ä¸€è¡Œï¼š
        - æ„å»º from_noise_levels / to_noise_levels
        - xs_pred[start_pos:] = sample_step(xs_pred[start_pos:], from, to)
     e. curr_pos += horizon
3. è§£ç  embedding â†’ token ids
```

**è°ƒåº¦çŸ©é˜µ**ï¼ˆ`_generate_scheduling_matrix`ï¼‰ï¼š
- **pyramid**ï¼š`scheduling_matrix[m, t] = sampling_timesteps + int(t * uncertainty_scale) - m`
- **full_sequence**ï¼šæ‰€æœ‰ token åŒæ­¥å»å™ª

### 5. DDPM/DDIM é‡‡æ ·ï¼š`_sample_step_seq()`

**å…³é”®ç‰¹æ€§**ï¼ˆå®Œå…¨å¯¹åº”åŸç‰ˆ `Diffusion.sample_step`ï¼‰ï¼š
- **Stabilization**ï¼š`noise_level == -1` æ—¶ï¼Œç”¨ `stabilization_level-1` é‡æ–° `q_sample` context
- **åªæ›´æ–° noise level ä¸‹é™çš„ä½ç½®**ï¼š`torch.where(curr == next, orig_x, x_pred)`
- **è°ƒåº¦ç´¢å¼•æ˜ å°„**ï¼š`[0, sampling_timesteps]` â†’ `[-1, timesteps-1]`

---

## âš ï¸ å…³é”®æ³¨æ„ç‚¹

### 1. ä¸åŸç‰ˆ Diffusion Forcing çš„å¯¹åº”å…³ç³»

| åŸç‰ˆï¼ˆdiffusion-forcingï¼‰ | å½“å‰å®ç°ï¼ˆnanoGPTï¼‰ |
|---------------------------|---------------------|
| `Diffusion.forward()` | `DFGPT.forward()` |
| `Diffusion.compute_loss_weights()` | `DFGPT._compute_loss_weight()` |
| `Diffusion.model_predictions()` | `DFGPT._df_model_predictions()` |
| `Diffusion.sample_step()` | `DFGPT._sample_step_seq()` |
| `Diffusion.ddim_sample_step()` | `DFGPT._ddim_sample_step_seq()` |
| `DiffusionForcingBase.validation_step()` | `DFGPT.generate_df()` |
| `DiffusionForcingBase._generate_scheduling_matrix()` | `DFGPT._generate_scheduling_matrix()` |

---

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### 1. Teacher Forcingï¼ˆæ ‡å‡† GPTï¼‰

```bash
python train.py \
    --config=config/train_shakespeare_char.py \
    --use_diffusion_forcing=False
```

### 2. Diffusion Forcing

```bash
python train.py \
    --config=config/train_shakespeare_char.py \
    --use_diffusion_forcing=True
```

### 3. é…ç½® Diffusion Forcing è¶…å‚

åœ¨ `GPTConfig` æˆ–é…ç½®æ–‡ä»¶ä¸­è®¾ç½®ï¼š

```python
df_timesteps = 32              # æ‰©æ•£æ—¶é—´æ­¥æ•°
df_sampling_timesteps = 32     # é‡‡æ ·æ—¶é—´æ­¥æ•°ï¼ˆâ‰¤ timestepsï¼‰
df_snr_clip = 5.0              # SNR å‰ªè£å€¼
df_cum_snr_decay = 0.95        # Fused SNR è¡°å‡ç³»æ•°
df_objective = "pred_noise"    # "pred_noise" / "pred_x0" / "pred_v"
df_beta_schedule = "cosine"    # "linear" / "cosine" / "sigmoid"
df_clip_noise = 5.0            # å™ªå£°å‰ªè£
df_ddim_eta = 0.0              # DDIM etaï¼ˆ0=ç¡®å®šæ€§ï¼Œ>0=éšæœºï¼‰
df_stabilization_level = 0     # Stabilization level
```

---

## ğŸ“ å¾…åŠäº‹é¡¹ / å¯é€‰æ”¹è¿›

1. **æ—¶é—´åµŒå…¥èåˆæ–¹å¼**ï¼šå¦‚éœ€å®Œå…¨ä¸€è‡´ï¼Œå¯æ”¹ä¸ºæ‹¼æ¥ + MLPï¼ˆå½“å‰æ˜¯ç®€å•ç›¸åŠ ï¼‰
2. **Guidance æœºåˆ¶**ï¼šå¯æ·»åŠ ç±»ä¼¼ `df_planning.goal_guidance()` çš„æ¡ä»¶å¼•å¯¼
3. **æ›´å¤šè°ƒåº¦æ¨¡å¼**ï¼štrapezoidã€autoregressive ç­‰
4. **è¯„ä¼°æŒ‡æ ‡**ï¼šæ·»åŠ å›°æƒ‘åº¦ã€BLEU ç­‰ï¼Œä¾¿äºå¯¹æ¯” Teacher Forcing vs Diffusion Forcing

---

## ğŸ“š å‚è€ƒ

- **åŸç‰ˆ Diffusion Forcing ä»“åº“**ï¼š`D:\05_Project\03_Python\toys\diffusion-forcing\algorithms\diffusion_forcing`
- **å…³é”®æ–‡ä»¶**ï¼š
  - `models/diffusion.py`ï¼šæ ¸å¿ƒ Diffusion ç±»
  - `models/transformer.py`ï¼šåºåˆ— Transformer
  - `df_base.py`ï¼šåŸºç¡€è®­ç»ƒ/é‡‡æ ·é€»è¾‘
  - `df_planning.py`ï¼šè§„åˆ’ä»»åŠ¡ï¼ˆå« Guidanceï¼‰

---

## ğŸ’¡ å¿«é€Ÿç†è§£è¦ç‚¹

1. **Diffusion Forcing = åœ¨ embedding ç©ºé—´åšæ‰©æ•£ï¼Œè€Œä¸æ˜¯ token ç©ºé—´**
2. **è®­ç»ƒæ—¶ï¼šå¹¶è¡Œå¤š tokenï¼Œæ¯ä¸ª token ç‹¬ç«‹ noise level**
3. **é‡‡æ ·æ—¶ï¼šè°ƒåº¦çŸ©é˜µ + æ»‘åŠ¨çª—å£ + å¤šæ­¥è¿­ä»£**
4. **ä¿æŒ nanoGPT é£æ ¼ï¼šå•æ–‡ä»¶ã€ç®€æ´ APIã€æœ€å°æ”¹åŠ¨**
5. **å®Œå…¨å¯¹åº”åŸç‰ˆé€»è¾‘ï¼šæ•°å­¦å…¬å¼ã€é‡‡æ ·æµç¨‹ã€è°ƒåº¦çŸ©é˜µéƒ½ä¸€è‡´**
```